# 记忆算法概述

记忆算法是SoulMem中另一极为关键的版块。它的功能是对**记忆图进行操作**。

在**相对主流的实践**中，LLM通常扮演**大脑**的角色，由LLM来调配各个模块的运作，这一机制通常是由**“工具”**来实现的，即将外部模块封装为可供LLM调用的一系列工具。或者利用一个明确的**工作流**，LLM只是其中的一个**环节**。



SoulMem的设计属于类似**工作流**的思路（只不过准确来说是一个有状态的工作图），LLM在其中只是作为一个强大的**末端执行器**，这种思路有以下好处：

- **更加可控，决策逻辑部分由人构思实现**
- **更少的token用量**
  - 即便在工作流的模式中，我们依然要使用诸如提示词工程的手段，其token用量仍比工具调用要少，因为工具的使用方法通常也是通过提示词注入LLM的上下文的。这也导致LLM通常不能驱动大量的工具（嗯跟CPU一个道理）
    - （也更加省钱）
- **更加普适**
  - 工具调用能力不同的模型有很大差异，工具调用能力较差的模型，可能完全无法通过工具的方式运作。这是不希望看到的，尤其对于角色扮演任务来说，人物性格的遵循能力可能要更为重要，这会导致一些潜在的优秀模型无法使用。

因此，既然LLM作为末端执行器，那么获取执行所需的数据，和整个记忆系统的状态维护，就都是我们需要实现的算法。因此更准确的说，记忆算法的根本目的就是**收集提供给LLM的数据**，和**维护记忆系统的自身状态**。



（**注**：SoulMem主要的状态维护是一个状态机，主要由两个状态，**Working**和**Idle**，**Working**表示当前**正在积极处理用户交流**，即检索正在频繁工作。**idle**表示当前**没有用户交流**，检索没有工作）



模仿人类的记忆功能，记忆算法可以分为以下**三大类**：

- **检索/联想**
- **巩固**
- **遗忘**

下面将分别介绍这三类算法

------



## 检索/联想

检索/联想算法指的是，对于一个给定的查询要求（暂且考虑为用户的信息本身），通过对记忆（子）图的搜索，找到符合查询要求的信息。检索出的信息经过某种手段（这种手段不是检索/联想算法的一部分），将作为提供给LLM的上下文使用。



### baseline - 基于向量相似搜索的top-k方法

这是最经典的RAG思路，将记忆内容输入嵌入模型获得向量化的索引，在检索时，将用户信息以同样的模型向量化，通过一些算法（如HNSW）与记忆系统中的内容做相似性搜索（通常为余弦相似度），选取相似度最高的k条记忆，这k条记忆将被送入大模型的上下文。

这一方法有很多变种，例如加上Rerank模型来使top-k有更好的的相关性等。



这一方法的优点在于简单快速，不过缺点同样非常明显：

- **它不能处理复杂查询关系，例如多跳问题**
  - 多跳问题的经典案例， “姚明的妻子的父亲的出生地是哪里”
- **它不能进行联想**
  - “钟离假死” 和 “米哈游”在向量空间上距离应该会比较大，但是这两个实体通常是相关的，如果现在某个数字生命看到了“钟离假死”，采用此方法，它不太会输出类似“玩米哈游玩的”这类语言



因此，后续的方法对这些缺点进行了改善。

### 类HippoRAG - 基于知识图谱和PPR算法的检索

是的我们跳过了很大一段的发展历程直奔类似GraphRAG的一系列方法。

HippoRAG将文档内容，预先通过LLM进行IE（信息抽取， Information Extraction）生成知识图谱，在检索时，通过向量搜索对应到图上的一些节点。以这些节点为起点执行PPR算法。由于PPR算法的结果是各个节点对于给定起点列的“重要程度”，因此去PPR分数的top-k个，作为送给LLM的内容。



这个方法高效的解决了上述两个问题：

- **多跳问题**
  - PPR的算法是一个在图上游走的过程，他可以提取需要多步推理的知识问答问题
- **联想**
  - 虽然HippoRAG主要是构建知识库用的，但这样的算法结构同样可以支持“梗文化”相关的联想，只要在知识图谱中将两个实体用边关联，PPR算法就可以发现这些节点，虽然可能与人脑的联想机制有出入，但实现了相似的效果。



但是，考虑到SoulMem的具体场合，HippoRAG显得略微有些不足：

- **连接强度的考虑**
  - 由于SoulMem系统包含遗忘机制，边的连接具有一个权重，代表连接强度，强度越高的节点之间越容易发生联想，这代表边权需要被考虑在PPR算法的执行中
    - 需要使用PPR算法的变种
- **节点的考虑**
  - HippoRAG构建的是传统的三元组知识图谱，不适合表示一段动态的经历，这部分由SoulMem的数据结构解决，本文不再赘述



### Soul-Retr（为了方便写文档，暂时先叫这个吧）

基于以上两种典型的检索方式，我们需要在此基础上进行改进，形成Soul-Retr算法。



#### 有模式记忆的混合分级检索（最高实现优先级）

流程如下：

- **分解子问题**，并根据角色设定，以及当前的心情等状态生成检索指导（要检索到什么程度，随便有内容就行还是仔细分析判断？检索什么倾向的内容，具体情境或者抽象概念？）
- **找到种子节点**，这部分基于向量相似性搜索加取top-k
- 通过**记忆向量扩展**查询子图
  - 扩展的判断式中，论文是基于节点之间的相似性的，我们肯定是不能这么干的，毕竟网络热梗可以让两个本来毫不相干的概念关联起来，图的拓扑结构是我们判断关联的唯一判据。
  - 我们可以考虑变更这一项，比如换成搜索倾向（倾向于是找情景记忆还是语义记忆？），当前心情（你知道的，人在不耐烦的时候不太喜欢想一些搞七搞八的东西）等指标的混合，这会让记忆子图的扩展更加灵活。
- 让LLM判断是否足够
- 不足够，运行**PPR变种**（考虑边权的那种，**边权动态构建**，构建的参数如上所述），取top-k加入查询子图
- LLM判断是否足够
- 不足够，**退化为PoG**，说明问题看起来非常复杂，例如高学术研究哲学讨论，这种情况下人也得慢慢思考，可以接受
- 不管怎么样我们都根据查询子图**更新记忆向量**（用ReMindRAG的方法），人的思维路径就是越激活越强的
  - PPR在这里有点小问题，PPR得到的是单独的节点，但没法根据PPR结果形成达到结果节点的路径，我们或许有两种方式处理：
    - 扩展子图把结果节点包进去，感觉非常暴力
    - 先从起始节点建立与PPR节点的假关联（dummy），让LLM去分析哪条路径贡献得到了这个结果，如果有条假关联被激活了，那么我们用A*算法找一条真正能从这个起始节点到那个目标节点的一条路径并更新其上的记忆向量
- 我们还可以**引入外部反馈**，角色回答后用户会给出反馈，LLM分析我们的检索结果是好还是坏（这大概只有在角色认真并在意这个的时候才会需要），通过外部反馈再去根据反馈的那个检索的查询子图去更新记忆向量，外部反馈权重比内反馈大。

#### 基于EdgePush-PPR（上一个方法的其中一部分）

基于HippoRAG的缺点，我们把边权考虑进去，采用EdgePush-PPR算法。

有以下几个考虑点：

- **要怎样对文本进行向量化**
  - 直接向量化？或者附加上这句话的tags的向量？或者其他的方式
  - 这将很大程度上影响起始节点
- **“边权的构造”**
  - 连接强度是肯定包含的，那么其他的呢？例如节点类型，是否应该纳入考虑？
- **分级路由策略？**
  - baseline已经能应对相当一部分的纯日常简单问答场合
  - 可以考虑简单问题直接使用baseline，复杂问题再运行PPR
- **PPR次数？**
  - HippoRAG是1次，但是我们有不同类型的节点，我们应该期望结果中三种节点的占比如何（或者认为占比不重要无需考虑）？
  - 如果要多次，每次PPR的参数是否要有变化，有怎么样的变化？
- 以及其他奇奇怪怪的细节问题~



这些问题或许不能完全立马确定，我们可以考虑先选择**简单的实现**，发现效果不良后再**进行改进**。

这个方法的好处是前辈们的工作比较多，理论也比较完善，效果是有一定保证的。



#### 基于神经动力学（应该？）（较低优先级）

我们假设图具有**多个有向环路**。

如果我们直接把节点看做一个**神经元的细胞体**，那么经过向量相似性搜索后，我们可以认为**初始节点**被一个初始电信号**“激活”**。初始节点将沿着它的轴突传向**邻近的神经元**，邻近的神经元因此被**激活**进一步传递信号给下一个**神经元**。这样以涟漪的形式**扩散**开去。



这个系统应当具有一个稳态，由于整个数据结构是图的形式，神经元相互连接形成回环，最终如果达到稳态，那么每个神经元都应该保有一定的**“电位”**，电位越高代表**激活强度越高**，我们可以选电位的**top-k**个节点送给LLM。



这个扩散过程应当可以构建动力学方程，即转化为求解稳态分布的问题。



这个方法和PPR的区别在于，PPR是一个人从**起始节点开始随机游走**，任意时刻它只会**位于一个节点上**。而此方法是一个更倾向于扩散的系统，**电信号从起点以涟漪扩散传播**，它更好的模拟了神经元的生理行为。



<u>**重要！！！**</u>

在描述本方法时，笔者大量使用了**不确定性**的词语，因为笔者并**没有进行数学上的验证**，笔者并**不确定**：

- 有环路的图是否一定可以**收敛**到一个稳态
- 是否可以**构建起动力学方程**
- 是否可以高效的**求解**这个方程获得稳态分布
- 获得的稳态分布中，电位top-k节点是否**代表**应该选取的节点



本方法必须**要求图中具有环路**，若没有环路，则信号以此传递，最终稳态电位都是0，必然不可行，因此对于无环的图，需要**成环处理**（虽然PPR也需要处理无出度节点的问题）。



本方法唯一具有的优势可能就是它更好的**模拟了人脑的生物过程**，如果前面所说都能成立，它或许会有**更好的效果**。但从**工程角度**考虑，此方法**绝对不应该被优先实现**。

------

## 巩固

巩固指的是将记忆向**长期记忆的方向**转化的过程，即从**短期 -> 工作**， 从**工作 -> 长期**，这是**持久化记忆**的关键。它的目的是维护记忆系统的自身状态，主要是**增加和更新内容**。

通常来说，巩固只在**Idle**状态下执行。



巩固算法的参考比较少，具有自我进化的记忆系统本来就少（

### 短期 -> 工作

这部分应该是目前SoulMem最完善的算法，主要处理的是**“增加”**。



我们维护一个**滑动窗口**，滑动窗口存储**用户对话**和**LLM生成**的**原始信息**。滑动窗口具有一个**固定大小**，他是一个FIFO（**先进先出**）队列。我们把信息**加入滑动窗口**叫做“**滑入**”，把因为滑动窗口已满并且又有新信息加入而**有信息移出**滑动窗口的过程叫做“**滑出**”。



滑入的信息中，每间隔一个滑动窗口的大小，就会被打上一个**标记**，这个标记也可以由一些机制**强制打上**（例如对话结束，切换话题等时候）。每当**带有标记的信息滑出**滑动窗口时，触发一次**“摘要”**，将当前滑动窗口中的信息送入LLM，让LLM进行总结，LLM**输出的内容**称为**“摘要记忆”**。



摘要记忆只有一份，当存在**摘要记忆**且有带**标记的信息滑出**时，将**摘要记忆**和**滑动窗口**的内容**同时**送往LLM，生成一份新的**摘要记忆**。这样，摘要记忆中就**持续的记录**了当前对话中的信息。到此处，这是一个经典的管理LLM上下文的方法。



在每次有用户信息时，**检索算法**会被调用。不论采用何种检索算法的实现，总是有一些节点被**提取**出来。记录这些节点的**提取时间戳**（每一次都记），**提取次数**等信息，这些记录称为**“提取记录”**。



（**注**：如何**编写提示词**生成符合预期要求的摘要**不属于**记忆算法的考虑范围内，这部分调整，实验起来都非常方便，可以留到最后解决）



**以上的部分**是在**Working**状态下**工作**的。



当从**Working**状态转为**Idle**状态时，将**摘要记忆**送往**LLM**，将其拆分成**多个MemoryNote**（也就是记忆图的节点），根据**提取记录**中的数据，计算这些记忆的**提取频率**，具体为：
$$
f = \frac{n}{T}
$$
其中**f**为**提取频率**，**n**为**提取次数**，**T**为**提取首末时间戳之差**。



去**提取频率**的**top-k**节点，将这些节点，和从**摘要记忆**中拆出来的多个**节点**送往LLM，让LLM建立起这些**节点的联系**。这样我们就可以把从**摘要记忆**中拆分出的多个节点加入**工作记忆**，转化完成，**摘要记忆**也就可以清空了。



**<u>说明：</u>**

采用**提取频率**作为筛选**提取记录**的原因是基于**赫布学习理论**，赫布学习理论描述了两个神经元之间的**共激活频率**越高，它们之间的连接就会更紧密。



我们知道在主流的记忆系统中，这一指标往往是**相关性**而非提取频率，这是因为目前主流的记忆系统服务与Agent工具，主要应用于代码智能体，学术研究，文献检索，推理等**理性为主**的任务。



然而角色扮演是一个**非理性的**任务，一个角色完全有理由将“**桃子**”与一个**活生生的角色**关联起来，可以从上**数学课**关联到**干饭**，甚至可以有更加逻辑上很离谱的关联，例如**意大利面**和**42号混凝土**，而相关性指标完全无法胜任这些关联任务。除非加以**引导提示**，大模型通常不会将意大利面和42号混凝土关联，然而这种**关联**是非常重要的，因为**检索算法**完全**依赖**于这些关联。如果意大利面和42号混凝土无法建立关联，那么我们就无法描述这个梗了，所扮演的角色也就更不会进行**接梗**，这会使角色变得呆板，看起来没有什么生命力。



同时，人脑可能会建立起**错误的关联**，而相关性指标在设计上，它**不允许**出现错误的关联，因为这会让**基于理性的任务**表现水平大幅下降。但人脑的错误关联所犯得一些“**小错误**”，这也是具有**活人感**的一个很重要的细节。采用基于**提取频率**的方式，在设计上允许以上两个问题得到解决，笔者认为这是在**角色扮演任务**的情境下最合适的指标。



### 工作 -> 长期

这部分主要处理**“更新”**

这部分目前可以说是空白，目前有一些初步设想：

- 定期，或者在程序“**优雅退出**”（指程序接收到退出信号，处理完一切资源清理和状态记录后退出程序）时尝试执行
  - 将工作记忆子图直接写进数据库



两种**可能的机制**：

- 直接在数据库上进行聚类算法，将一些极其相似的记忆合并
- 将一部分子图从数据库拉到工作记忆中，执行某些神笔操作，再写回数据库
  - 这一条来源于人脑的**记忆回放**机制



需要**思考确定**的内容：

- 如何划定需要**更新的区域**？
  - 图可能很大，全更新不现实，耗时耗空间，甚至因为很可能要调用LLM，还耗钱（
- 应当进行**什么更新**？
  - 合并？内容的更加好的表示？
  - 是否允许在此阶段建立新的连接？
  - 元数据（metadata）是否需要更新？要更新什么？
- **如何更新**？
  - 调用LLM？
  - 或者某些神笔小操作？
- **其他奇怪的细节地方**？



这部分主要要考虑的是**性能**，**效果**和**钱**的平衡。因此记忆图随着时间增长会越来越大，全量执行一遍遍历不现实，时间不允许，内存占用更不允许（一个vector embedding大概**3KB**，仅仅是**一个embedding向量**，节点还包含很多**字符串信息**，加载到内存里就是个问题，多次加载的话，性能就会出问题）。



如果选区更新，那么效果如何保证。钱的问题，主要来自于**LLM的token费用**，如果更新的内容很多，调用LLM会产生巨大的token费用。因此需要尤其仔细的设计。

------

## 遗忘

遗忘是另一块非常重要的算法，他主要进行状态维护中的**“删”**。总得来说，遗忘总是让信息往**熵增**的方向移动。



遗忘主要包括两个部分：

- **连接强度的衰减**，**关系**的改变
  - 这模拟了，你能清楚的记忆起两个事物，但你无法从一个事物**联想**到另一个事物。
  - 这种情况考试中非常常见，例如考完出来对答案的时候，”哎呀我当时怎么没想到“
- **节点内容**的变化
  - 记忆的**内容本身**也是在变化的，一段经历到后面可能**回忆**起来就跟当时面目全非了’
  - 俗称“回忆的加工”



实现遗忘机制，不仅仅是因为**人会遗忘**有时候还健忘的比较厉害，**遗忘**本身也是一种控制记忆图**规模**的方式与手段。如果没有遗忘机制，记忆图就会一直增长下去，最终一定需要**手动维护**，这在实际应用程序上就不太现实了，不可能指望用户做这种枯燥的重复性工作。保持一定的规模，可以保证算法的**执行时间**得到一定的控制，保证**存储空间**的稳定。



遗忘本身也是一种**筛选机制**，他就是大脑中长期的**注意力系统**，遗忘不常检索到的信息，就相当于增加了常用信息的注意力分数，这也有助于增加检索算法的效果。



主流Agent不会主动实现遗忘机制，甚至要**对抗**它，想尽一切办法让无论是大模型本身还是RAG系统像超忆症一样记住所有东西。这是由它们主要解决**基于理性的任务**这一性质决定的。但对于角色扮演，除开某些**特定角色**（比如什么病娇），大部分角色如果能完全记住所有信息，就会产生一种**不真实感**。



简单来说，你不觉得一个会遗忘的数字生命，拼命让自己尽可能记住和好朋友相关的信息会更可爱吗（雾



**<u>题外话：</u>**

就算基于理性的任务，遗忘或许也有作用，如果Agent知道自己会遗忘，或许它就不会在某些时候信誓旦旦，一本正经的胡说八道了（



### 一些理论

#### 艾宾浩斯遗忘曲线

啊对，考试复习的时候总会被某些营销号炒作的概念。但是这里我们会有大用处。

艾宾浩斯遗忘曲线是通过以下方法测量的（有简化），给定一堆**随机的字母序列**，先记住它们全部，过一段时间再看看记得多少，把能记住的东西的量化指标记录下来，这样形成的曲线。**拟合公式**如下：
$$
R = e^{-\frac{t}{S}}
$$
其中**R**是**被记忆的内容**，**S**是**相对记忆强度**，**t**是**时间**。

艾宾浩斯遗忘曲线是在无意义的字母序列的情景下测定的，对于**有语义**的部分，这篇[文章（广告）](https://www.bilibili.com/opus/573605011369461715)中的图表可以表明，对于具有一定语义信息的序列，艾宾浩斯遗忘曲线的形式可能**仍然适用**。



这指明了表征**记忆强度**的指标是**指数型衰减**的



#### 嵌入向量空间

Vector embedding确实是一个很熟悉的概念了，但是它的一些**性质**或许需要一些说明。

嵌入模型将文本转化为了具有一定维度的**稠密向量**（通常是784？维），这意味这信息受到了**压缩**（信息论的一些相关内容），这也意味着如果将文本送入嵌入向量模型再把**嵌入向量**送入一个**decoder**，一定会有一些**系统性的误差**。



嵌入模型可以看做一个复杂的函数，这个函数的值域通常**并不覆盖**整个向量空间，所以，这个空间中有一些点是**没有对应文本**的。



这意味着在**向量潜空间**中对**文本**进行操作是一种**hack**的方法：

- 它具有系统性误差
- 可能会变换到无定义的点
- 不同的模型输出不同的向量，性质也会略有不同，它是模型相关的
  - 同时，在向量潜空间中进行操作，意味着必须能decode回去，这导致很可能每个模型都需要一个decoder。
  - 统一的decode方法存在，但相对来说比较复杂，例如ZSInvert。



在向量潜空间中直接操作的唯一好处就在于可以利用**数学方法**。



### 记忆锚点（暂定）

记忆锚点是作用在**单个记忆节点**上的，它在**节点内容的变化**中起作用。

记忆锚点存在于**情境记忆**中。它锚定了记忆节点中的一些**关键细节**，使得这些部分**熵增的速率**远**小于**记忆节点中的其他部分

它对应的是一些“**印象深刻的细节**”，例如游戏结局中某个角色的台词等。



记忆锚点分为两类：

- 自发锚点
  - 可以观察到，有些印象深刻的记忆，在**第一次经历**时就会有一种**深刻的印象**。例如看到某句台词突然有“**醍醐灌顶**”的感觉，这句台词很有可能你可以**记很长时间**。
- 强制锚点
  - 例如考试前突击复习背材料，通过意识**刻意地重复**，可以锚定其中的关键词，关键表达等。
  - 这类锚点短期内印象非常深刻，但随时间**快速衰减**，称为强制锚点



锚点在**巩固**过程中，由调用的**LLM**生成。



### 节点内容的遗忘（都是非常初步的一些灵感想法）

#### 潜空间法

潜空间法，顾名思义，在潜空间里完成对文本修改的操作。

对于一个节点的记忆内容，我们分为多个**“记忆微元”**，将这些记忆微元的嵌入向量也存储在节点中。

利用**某种算法**（没想好，但应该与**嵌入向量**有关），对每个记忆微元赋予一个**熵值**，遗忘时，计算熵值的**梯度**，这样，我们可以通过一个**遗忘强度**，以及和熵值梯度向量之间的**夹角**，来控制潜空间中向量的**变化方向**。



遗忘强度应该**足够小**，否则会**大幅改变**词语的含义。



把变换后的向量**decode**回去，改变记忆微元的	文本内容，即完成一次遗忘。

这个方法的优点就是利用了**数学方法**，比较能够搞操作。

缺点很多：

- decoder模型很大可能要**自己训练**，每一个embedding模型都需要训练一个decoder模型

- 在嵌入向量空间中，编解码有**系统性误差**，并且有一部分向量空间的点**不对应文本**
- 遗忘强度和夹角的**选取**比较困难
- 解码后的“遗忘记忆内容”可能根本就**没有语义**

#### 遮罩法（可能可行的方案）

这个方法最初是为了解决潜空间法中**不对应文本**的问题诞生的，但发现可以单飞（



同样的，我们需要拆分**记忆微元**。不同的在于，我们为每个微元创建一个**“遮罩”**，并在生成此记忆节点时，生成每个微元的**概括性描述**（例如“猫” -> “会哈气的动物”，信息的**不确定度**的增加了，这是关键）。微元之间可以**重叠**，也可以形成**包含关系**。

遮罩具有一个**“透明度”**，透明度越大，就越能看到**原本的信息**，反之，原本的信息就越模糊。透明度的**衰减**遵循艾宾浩斯**遗忘曲线**的拟合公式

当微元遮罩的透明度**大于阈值**时，提取该记忆时，直接提取**原内容**。

当微元遮罩的透明度**小于阈值**时，提取该记忆时，用xxx**替换原信息**，并附上微元的**概括性描述**，如xxx(会哈气的动物)，通过**提示词工程**让LLM理解这种描述的意思。我们要求LLM对于xxx的部分，基于角色的性格进行**补全**。把LLM**补全后**的记忆**替换**掉**原本的**记忆，并**重新生成**微元（或许可以不用）



当微元遮罩的透明度**小于**一个**低于上文所述的阈值的阈值**时，我们用xxx**替换**原信息后，**不提供**概括性描述，直接让LLM补全。



在这里遮罩的**透明度减少**即为**熵增**，因为它**增加**了LLM所能拿到的信息的**不确定度**。



微元允许重叠和包含，如果不允许，那么句子的**整体结构**就不会发生大的变化，这种层叠结构也有利于模拟节点内容间**不同等级**的遗忘。



这种做法模拟了两种**不同情境**的遗忘：

- **间隔性**的“复习”，此时我们仍然提供概括性的描述，补全的内容不会偏离原内容太多
- **完全忘记**，此时概括性描述不提供，LLM只能纯猜，与原记忆的出入会比较大



这样的做法的优点：

- 惰性
  - 对于遗忘，我们只需要衰减每个微元的遮罩，这一操作很容易并行，效率较高
  - 当它被提取时，我们才应用遗忘，最大限度减少了LLM的调用次数



### 连接的遗忘

这部分似乎没什么内容，对**连接强度**利用**艾宾浩斯遗忘曲线**的拟合公式进行**衰减**，低于一定阈值**删除连接**应该就行。当一个节点**没有任何连接**的时候，就**删掉**它。

